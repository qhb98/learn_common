{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "来源: https://leetcode.cn/circle/discuss/gSAs2d/\n",
    "\n",
    "字节跳动一面:\n",
    "    LR\n",
    "    SVM敏感程度 对非线性数据怎么处理\n",
    "    LR和SVM的对比\n",
    "    DT决策树如何处理缺失值, 是否敏感\n",
    "    RF随机森林的随机性体现在哪里\n",
    "    LSTM 和 RNN\n",
    "    正则化的本质和作用\n",
    "    如何衡量特征对于结果的贡献\n",
    "    XGBoost 和GBDT RF的异同点\n",
    "    梯度下降\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ================ 根据前辈面经查漏补缺 ===================================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 梯度下降\n",
    "参考链接: https://zhuanlan.zhihu.com/p/357963858\n",
    "\n",
    "梯度下降是一个使损失函数越来越小的优化算法, 在求解约束优化问题时, gradient descent 是最常采用的方法之一.\n",
    "\n",
    "梯度:\n",
    "    在微积分里对多元函数的参数求偏导, 把求得的各个参数的偏导数以向量的形式写出来, 就是梯度.\n",
    "    几何上来讲就是函数变化增加最快的地方, 沿着梯度向量的方向更加容易找到函数的最大值.\n",
    "    沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解\n",
    "\n",
    "随机梯度下降, mini-batch梯度下降和批量梯度下降:\n",
    "    对整个训练集进行梯度下降的时候必须处理整个训练数据集, 然后才能进行一步梯度下降, 即每一步梯度下降都要对\n",
    "    整个训练集进行一次处理, 也就是一个epoch. 但是如果训练数据集过大, 处理的速度就会很慢, 而且不可能一次性地载入到内存或者显存中, 所以会把大数据集分成小数据集, 一部分一部分地训练, 这个训练子集称为mini-batch.\n",
    "    对于Mini-batch梯度下降法, 一个epoch可以进行Mini-batch的个数次梯度下降, 即每一小批的数据输入模型训练后就对训练的参数进行梯度下降更新一次.\n",
    "\n",
    "    batch gradient descent:\n",
    "        BGD就是每个epoch计算所有样本的loss, 进而计算梯度进行反向传播和参数更新.\n",
    "        优点:\n",
    "            每个epoch通过所有样本计算loss, 这样计算出的loss更能表示当前分类器在整个训练集的表现, 得到的梯度方向更能代表全局极小值点的方向.\n",
    "        缺点:\n",
    "            每次都需要用所有样本计算loss, 在样本数量非常大的时候只能有限的并行计算, 效率低下.\n",
    "\n",
    "    stochastic gradient descent:\n",
    "        SGD每次迭代计算单个样本的损失并进行梯度下降更新参数, 这样每轮epoch可以进行m次的参数更新.\n",
    "        优点:\n",
    "            参数更新速度加快\n",
    "        缺点:\n",
    "            计算量大且无法并行\n",
    "            容易陷入局部最优导致模型准确率下降, 单个的loss无法代替全局的loss\n",
    "\n",
    "    mini-batch gradient descent:\n",
    "        小批量梯度下降将所有的训练样本划分到batch size个mini-batch中, 每个mini-batch包含batch size个训练样本, 每个iteration计算一个mini-batch中的样本的loss, 进而进行梯度下降和参数更新.\n",
    "        batch size 一般取 2的整数次方\n",
    "\n",
    "\n",
    "常见的梯度下降算法:\n",
    "    随机梯度下降 - SGD一般取值0.1 0.01 0.001, 带有动量momentum的算法\n",
    "    滑动平均梯度下降RMSProp - root mean square prop可以减小某些维度梯度更新波动较大的情况, 加快梯度下降的速度\n",
    "    Adam - 将momentum 和 rmsprop结合起来形成的一种适用于不同深度学习结构的优化算法\n",
    "\n",
    "\n",
    "RMSProp:\n",
    "    通过累积各个变量的梯度的平方R, 然后用每个变量的梯度除以R, 即可 有效缓解变量间的梯度差异.\n",
    "\n",
    "    1. 初始化 学习参数 学习率lr 平滑常数/衰减速率alpha  梯度的平方等参数\n",
    "    2. while 没有停止训练 do\n",
    "    3.    计算梯度\n",
    "    4.    累积梯度的平方 r_x = alpha * r_x + (1 - alpha) * (g_x)^2\n",
    "    5.    更新可学习参数\n",
    "    6. end while\n",
    "\n",
    "Adam:\n",
    "    在RMSProp中, 梯度的平方是通过平滑常数平滑得到的, 但是并没有对梯度本身做平滑处理.\n",
    "    在Adam中, 对梯度本身也做了平滑.\n",
    "\n",
    "    1. 初始化 平滑常数beta1 beta2 学习率lr 平滑常数/衰减速率alpha  梯度的平方等参数\n",
    "    2. while 没有停止训练 do\n",
    "    3.    计算梯度\n",
    "    4.    累积梯度 m_t = beta1 * m_t-1 + (1 - beta1) * g_t\n",
    "    5.    累积梯度的平方 v_t = beta_2 * v_t-1 + (1 - beta2) * (g_t)^2\n",
    "    6.    偏差纠正 new m_t = old m_t / 1 - (beta1)^t\n",
    "    7.    偏差纠正 new v_t = old v_t / 1 - (beta2)^t\n",
    "    8.    更新可学习参数\n",
    "    9. end while\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}