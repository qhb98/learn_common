{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "来源: https://blog.csdn.net/lifeng_math/article/details/51424159\n",
    "\n",
    "阿里巴巴技术一面\n",
    "    LR SVM RF 决策树 的主要思想、优缺点和使用场景, 具体实现\n",
    "\n",
    "    hadoop mapreduce\n",
    "\n",
    "\n",
    "阿里巴巴技术二面\n",
    "    海量数据的匹配、搜索、查找问题\n",
    "\n",
    "\n",
    "大众点评一面\n",
    "    EM算法 快速排序算法的思想\n",
    "\n",
    "\n",
    "普林科技一面\n",
    "    精确度 准确度 召回率  F1值 ROC曲线\n",
    "\n",
    "\n",
    "普林科技二面\n",
    "    BP神经网络\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ================ 根据前辈面经查漏补缺 ===================================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LR 线性回归 linear regression\n",
    "\n",
    "属于监督学习回归类算法的一种, 用于评估自变量X和因变量Y之间的一种线性关系, 有一元线性回归和多元线性回归.\n",
    "\n",
    "通俗来讲, 就是将真实的数据, 映射到坐标轴中, 这些数据在坐标轴中, 呈现偏向线性的形状, 然后构建一个函数, 使得\n",
    "这个函数对应的数据尽量接近真实数据/尽量让所有的点距离我们构建出的函数所呈现在坐标轴上的线的差距最小.\n",
    "\n",
    "算法思想解析:\n",
    "    从简单的一元线性回归开始, y_预测 = w * x + b, 但是这个预测值和x是从样本中获取的, w的值还不知道, 所以需要求出这个w的值.\n",
    "    由于实际情况下, 会有很多的额外的影响因素对样本造成影响, 即噪声. 因为噪声和其他因素的存在, 会导致预测值和实际值存在误差.\n",
    "    而每个样本的误差都是独立同分布的, 根据中心极限定理, 这些误差符合正态分布.\n",
    "    另外, 给定一个x w, 得到的预测值越接近真实值, 那么样本的概率就越大; 而误差越小, 样本的概率也就越大. 所以, 样本\n",
    "    的概率和误差是一样的, 符合均值为0, 方差为theta^2的正态分布.\n",
    "    所以目标就是求损失函数最小的时候, 通过最小二乘法去求出最终的w的公式为 w = (X^T*X)^-1 * X^T * y\n",
    "    故, 只要将大量的样本和对应的标签传入到这个公式, 去求出w和b, 就可以实现预测\n",
    "\n",
    "过拟合:\n",
    "    过拟合是指一个假设在训练数据上能够获得比其他假设更好的拟合, 但是在测试数据集上却不能很好地拟合数据,\n",
    "    此时认为这个假设出现了过拟合的现象. 模型过于复杂.\n",
    "\n",
    "    原因: 特征过多, 存在一些嘈杂特征, 模型过于复杂是因为模型尝试去兼顾各个测试数据点\n",
    "    解决办法:\n",
    "        重新清洗数据\n",
    "        增大数据的训练量\n",
    "        正则化: 就是在cost function中加入一项正则化项, 惩罚模型的复杂度\n",
    "            L1正则 - 使得其中一些W的值直接为0, 删除这个特征的影响 L2正则 - 使得一些W的值很小, 接近0, 削弱某个特征的影响\n",
    "        减少特征维度, 防止维度灾难\n",
    "            维度灾难 - 随着维度的增加, 分类器的性能逐步上升, 到达某点后, 性能逐渐下降\n",
    "            造成维数灾难的原因 - 随着维度的提高, 样本的密度也在下降, 在这种情况下确实很容易找到一个超平面将目标分开, 然而当我们将高维空间向低维空间投影, 高维空间隐藏的问题将会显现出来, 高维度空间训练形成的线性分类器, 相当于在低维空间的一个复杂的非线性分类器, 这种分类器过多地强调了训练集的准确率以至于对一些错误/异常的数据也进行了学习, 而正确的数据却无法覆盖整个特征空间.\n",
    "        对应的情况:\n",
    "            high variance 高方差\n",
    "\n",
    "\n",
    "欠拟合:\n",
    "    一个假设在训练数据上不能获得更好的拟合, 并且在测试数据集上也不能很好地拟合数据, 此时认为这个假设出现了欠拟合的现象. 模型过于简单.\n",
    "\n",
    "    原因分析: 学习到的数据特征过少\n",
    "    解决办法:\n",
    "        添加其他特征项(组合, 泛化, 相关性, 上下文特征, 平台特征等)\n",
    "        添加多项式特征\n",
    "    对应的情况:\n",
    "        high bias 高偏差\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集R^2：1.0\n",
      "测试集R^2：1.0\n",
      "参数结果为:[[5.]]\n"
     ]
    }
   ],
   "source": [
    "# 代码样例Demo1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0,20,500).reshape(-1, 1)\n",
    "y = 5*x + 7\n",
    "# 创建线性回归类的对象，可以用来实现回归的任务。该类底层就是使用最小二乘法来求解的。\n",
    "lr = LinearRegression()\n",
    "# 传入x（训练样本），y（训练标签），将有限的数据分为俩部分，一部分训练，一部分测试\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "# 训练\n",
    "lr.fit(train_X, train_y)\n",
    "# 传入测试样本进行测试\n",
    "result = lr.predict(test_X)\n",
    "# 检查训练的效果\n",
    "print(\"训练集R^2：{}\".format(lr.score(train_X, train_y)))\n",
    "# 检查 测试的效果，1为满分，0为最低分（理论上最低分，实际最低分，可以是负无穷）\n",
    "print(\"测试集R^2：{}\".format(lr.score(test_X, test_y)))\n",
    "# 输出训练的参数结果\n",
    "print(\"参数的计算结果为:{}\".format(lr.coef_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM 支持向量机 support vector machines\n",
    "\n",
    "参考链接:\n",
    "https://blog.csdn.net/a857553315/article/details/79586846\n",
    "\n",
    "\n",
    "SVM学习的基本想法是求解能够正确划分训练数据集并且几何 间隔最大(使其有别于感知机) 的分离超平面.\n",
    "(1) 当训练样本线性可分, 通过硬间隔最大化学习一个线性分类器, 即线性可分SVM\n",
    "(2) 当训练数据近似线性可分, 引入松弛变量, 通过软间隔最大化, 学习一个线性分类器, 即线性支持向量机\n",
    "(3) 当训练数据线性不可分, 通过使用核技巧及软间隔最大化, 学习非线性支持向量机\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RF\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 决策树\n",
    "\n",
    "\n",
    "决策树对于缺失值的处理不敏感 https://blog.csdn.net/weixin_43935696/article/details/126317571\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 精确度 准确度 召回率  F1值 ROC曲线\n",
    "\n",
    "TP 预测为正, 实际为正\n",
    "FP 预测为正, 实际为负\n",
    "FN 预测为负, 实际为正\n",
    "TN 预测为负, 实际为负\n",
    "\n",
    "\n",
    "精确度precision/查准率:\n",
    "    以预测结果为判断依据, 预测为正例的样本中预测正确的比例.\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "准确率accuracy:\n",
    "    准确率是正例和负例中预测正确数量占总数量的比例, 即所有预测样本中预测准确的占比.\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "召回率recall/查全率:\n",
    "    以实际样本为判断依据, 实际为正例的样本中, 被预测正确的正例占总实际正例样本的比例. 即评估所有实际正例是否被预测出来的覆盖率占比.\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "F1值:\n",
    "    F1值就是中和了精确率和召回率的指标\n",
    "    F1 = 2PR / (P + R) = 2TP / (2TP + FP + FN)\n",
    "    当P和R同时为1, F1=1. 当一个很大另一个很小, 比如P=1, R~0, 此时F1~0. 分子2PR的2完全是为了使最终的取值在0-1之间, 进行区间放大, 无实际意义.\n",
    "\n",
    "\n",
    "1. 什么情况下精确率很高但召回率很低?\n",
    "    一个极端的例子. 假如黑球有3个, 只预测出了1号球是黑球, 其余两个球为白球\n",
    "    此时精确率为 precision = 1 / (1 + 0) = 1\n",
    "    但是召回率为 recall = 1 / (1 + 2)\n",
    "2. 什么情况下召回率很高但精确率很低?\n",
    "    如果10个球都预测为黑球, 此时所有实际为黑球都被预测正确了, 但实际只有3个黑球, 7个白球.\n",
    "    召回率 recall = 10 / (10 + 0) = 1\n",
    "    精确率 precision = 3 / (3 + 7)\n",
    "\n",
    "ROC (receiver operating characteristic curve) 曲线 接受者特征曲线:\n",
    "    ROC曲线是反映敏感性和特异性连续变量的综合指标, roc曲线上的每个点反映着对同一信号刺激的感受性\n",
    "    横轴是FPR, 表示预测为正但实际为负的样本占所有负例样本的比例,\n",
    "    纵坐标是TPR, 表示预测为正且实际也为正的样本占所有正例样本的比例.\n",
    "\n",
    "    ROC曲线有个很好的特性: 当测试集中的正负样本的分布变化时, ROC曲线可以保持不变. 在实际的数据集中经常会出现类\n",
    "    不平衡class imbalance 现象, 而且测试数据中的正负样本的分布也可能会随着时间而变化. 但ROC曲线一种非常稳定.\n",
    "\n",
    "AUC area under curve:\n",
    "    是ROC曲线下的面积, 取值范围一般在0.5-1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}